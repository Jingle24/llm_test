{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: multi-qa-mpnet-base-dot-v1\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from pptx import Presentation\n",
    "from docx import Document\n",
    "import logging\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "import yaml  \n",
    "import torch\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# Download NLTK data for sentence tokenization\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Retrieve configurations from environment variables\n",
    "API_TYPE = os.environ.get(\"API_TYPE\")\n",
    "DEEPSEEK_API_KEY = os.environ.get(\"DEEPSEEK_API_KEY\")\n",
    "DEEPSEEK_ENDPOINT = os.environ.get(\"DEEPSEEK_ENDPOINT\")\n",
    "AZURE_OPENAI_KEY = os.environ.get(\"AZURE_OPENAI_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_VERSION = os.environ.get(\"AZURE_OPENAI_VERSION\")\n",
    "AZURE_OPENAI_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "\n",
    "# Validate configurations\n",
    "if API_TYPE == \"deepseek\":\n",
    "    if not DEEPSEEK_API_KEY or not DEEPSEEK_ENDPOINT:\n",
    "        raise ValueError(\"DEEPSEEK_API_KEY and DEEPSEEK_ENDPOINT must be set\")\n",
    "elif API_TYPE == \"azure\":\n",
    "    if not all([AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_VERSION, AZURE_OPENAI_DEPLOYMENT]):\n",
    "        raise ValueError(\"All Azure OpenAI configurations must be set\")\n",
    "else:\n",
    "    raise ValueError(\"API_TYPE must be either 'deepseek' or 'azure'\")\n",
    "\n",
    "# Initialize Azure OpenAI client if using Azure\n",
    "if API_TYPE == \"azure\":\n",
    "    azure_client = AzureOpenAI(\n",
    "        api_key=AZURE_OPENAI_KEY,\n",
    "        api_version=AZURE_OPENAI_VERSION,\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    )\n",
    "\n",
    "# Initialize embedding model\n",
    "embedder = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "num_retrieved_indices = 5 # k\n",
    "\n",
    "# Global variables for indexing\n",
    "documents = []\n",
    "embeddings = []\n",
    "chunk_metadata = []\n",
    "faiss_index = None\n",
    "INDEX_INITIALIZED = False\n",
    "\n",
    "\n",
    "\n",
    "# Store conversation history (for multi-turn dialogue)\n",
    "conversation_history = []\n",
    "\n",
    "def PDFLoader(file_path):\n",
    "    logger.info(f\"Attempting to read PDF: {file_path}\")\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = []\n",
    "    for page in loader.lazy_load():\n",
    "        pages.append(page)\n",
    "    logger.info('PDF file loaded.')\n",
    "    return pages\n",
    "\n",
    "\n",
    "# Document processing functions with page tracking\n",
    "def extract_text_from_pdf(file_path):\n",
    "    pages = PDFLoader(file_path)\n",
    "    try:\n",
    "        chunks_with_pages = []\n",
    "        for i in range(len(pages)):\n",
    "            page_num = i + 1\n",
    "            text = pages[i].page_content\n",
    "            chunks = chunk_text(text)\n",
    "            for chunk in chunks:\n",
    "                chunks_with_pages.append({\n",
    "                                'text': chunk,\n",
    "                                'page': page_num\n",
    "                            })\n",
    "        logger.info(f\"Successfully read {len(chunks_with_pages)} chunks from {file_path}\")\n",
    "        return chunks_with_pages\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading PDF {file_path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "    # try:\n",
    "    #     with open(file_path, 'rb') as file:\n",
    "    #         pdf_reader = PyPDF2.PdfReader(file)\n",
    "    #         chunks_with_pages = []\n",
    "    #         for page_num, page in enumerate(pdf_reader.pages, start=1):\n",
    "    #             text = page.extract_text() + \"\\n\"\n",
    "    #             if text.strip():\n",
    "    #                 chunks = chunk_text(text.strip())\n",
    "    #                 for chunk in chunks:\n",
    "    #                     chunks_with_pages.append({\n",
    "    #                         'text': chunk,\n",
    "    #                         'page': page_num\n",
    "    #                     })\n",
    "    #         logger.info(f\"Successfully read {len(chunks_with_pages)} chunks from {file_path}\")\n",
    "    #         return chunks_with_pages\n",
    "    # except Exception as e:\n",
    "    #     logger.error(f\"Error reading PDF {file_path}: {str(e)}\")\n",
    "    #     return []\n",
    "\n",
    "def chunk_text(text, max_length=1000, min_length=300):\n",
    "    \"\"\"Split text into chunks with accurate length calculation and optimized merging.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to be chunked\n",
    "        max_length: Maximum character length per chunk (default: 1000)\n",
    "        min_length: Minimum character length for final chunks (default: 300)\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks meeting length requirements\n",
    "    \"\"\"\n",
    "    if not text.strip():\n",
    "        logger.warning(\"Empty text provided to chunk_text\")\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sent_tokenize(text):\n",
    "        sentence_length = len(sentence)\n",
    "        space_length = 1 if current_chunk else 0  # Space between sentences\n",
    "        \n",
    "        # Calculate potential new length\n",
    "        new_length = current_length + space_length + sentence_length\n",
    "        \n",
    "        if current_chunk and new_length > max_length:\n",
    "            # Finalize current chunk if meets minimum length\n",
    "            if current_length >= min_length:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "            # Else keep accumulating even if over max_length\n",
    "            \n",
    "        # Add sentence to current chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += space_length + sentence_length\n",
    "\n",
    "    # Handle remaining text\n",
    "    if current_chunk:\n",
    "        final_chunk = \" \".join(current_chunk)\n",
    "        # Merge small final chunk with previous if needed\n",
    "        if len(final_chunk) < min_length and chunks:\n",
    "            chunks[-1] += \" \" + final_chunk\n",
    "            # Split back if merge caused overflow\n",
    "            if len(chunks[-1]) > max_length:\n",
    "                last_chunk = chunks.pop()\n",
    "                chunks.extend([last_chunk[:max_length], last_chunk[max_length:]])\n",
    "        else:\n",
    "            chunks.append(final_chunk)\n",
    "\n",
    "    # Post-process to ensure all chunks meet length requirements\n",
    "    final_chunks = []\n",
    "    for chunk in chunks:\n",
    "        while len(chunk) > max_length:\n",
    "            final_chunks.append(chunk[:max_length])\n",
    "            chunk = chunk[max_length:]\n",
    "        if chunk:\n",
    "            final_chunks.append(chunk)\n",
    "    \n",
    "    # Log chunk details\n",
    "    logger.info('='*10 + f\"Created {len(chunks)} chunks from {len(text)} characters\" + \"=\"*10)\n",
    "\n",
    "    # logger.info(f\"Created {len(final_chunks)} chunks from {len(text)} characters\")\n",
    "    for idx, chunk in enumerate(final_chunks):\n",
    "        logger.info(f\"Chunk {idx}: {len(chunk):4} chars | Start: {chunk[:120].strip()}\")\n",
    "        logger.info('--'*20)      \n",
    "    \n",
    "    \n",
    "    return final_chunks\n",
    "\n",
    "# Function to initialize the FAISS index\n",
    "def initialize_index():\n",
    "    global documents, embeddings, chunk_metadata, faiss_index, INDEX_INITIALIZED\n",
    "    if INDEX_INITIALIZED:\n",
    "        logger.info(\"Index already initialized, skipping...\")\n",
    "        return\n",
    "    \n",
    "    documents_dir = \"./documents\"\n",
    "    \n",
    "    if not os.path.exists(documents_dir):\n",
    "        os.makedirs(documents_dir)\n",
    "        logger.warning(f\"Created empty documents directory: {documents_dir}\")\n",
    "    \n",
    "    documents = []\n",
    "    embeddings = []\n",
    "    chunk_metadata = []\n",
    "    \n",
    "    file_extractors = {\n",
    "        # '.txt': extract_text_from_txt,\n",
    "        '.pdf': extract_text_from_pdf,\n",
    "        # '.pptx': extract_text_from_ppt,\n",
    "        # '.docx': extract_text_from_docx\n",
    "    }\n",
    "    \n",
    "    logger.info(\"Starting document indexing...\")\n",
    "    for root, dirs, files in os.walk(documents_dir):\n",
    "        for filename in files:\n",
    "            logger.info(f'process for {filename}')\n",
    "            ext = Path(filename).suffix.lower()\n",
    "            if ext in file_extractors:\n",
    "                filepath = os.path.join(root, filename)\n",
    "                logger.info(f'process for {filepath}')\n",
    "                chunks_with_pages = file_extractors[ext](filepath)\n",
    "                logger.info(f'total pages extracted: {len(chunks_with_pages)}')\n",
    "                for i, chunk_info in enumerate(chunks_with_pages):\n",
    "                    documents.append(chunk_info['text'])\n",
    "                    chunk_metadata.append({\n",
    "                        'filepath': filepath,\n",
    "                        'chunk_index': i,\n",
    "                        'original_text': chunk_info['text'],\n",
    "                        'page': chunk_info['page']\n",
    "                    })\n",
    "    \n",
    "    if documents:\n",
    "        logger.info(f\"Generating embeddings for {len(documents)} document chunks\")\n",
    "        logger.info(f\"Sample document chunk: {documents[0][:100]}...\")\n",
    "        embeddings = embedder.encode(documents, show_progress_bar=True)\n",
    "        embeddings = np.array(embeddings).astype('float32')\n",
    "        logger.info(f\"Generated embeddings - shape: {embeddings.shape}, dtype: {embeddings.dtype}\")\n",
    "    \n",
    "        dimension = embeddings.shape[1]\n",
    "        logger.info(f\"Creating FAISS index with dimension {dimension}\")\n",
    "        faiss_index = faiss.IndexFlatL2(dimension)\n",
    "        faiss_index.add(embeddings)\n",
    "        logger.info(f\"Indexed {len(documents)} document chunks\")\n",
    "        logger.info(f\"FAISS index size: {faiss_index.ntotal}\")\n",
    "        if len(documents) != len(chunk_metadata):\n",
    "            logger.error(f\"Mismatch between documents ({len(documents)}) and chunk_metadata ({len(chunk_metadata)})\")\n",
    "    else:\n",
    "        logger.warning(\"No documents found to index. Please add files to the 'documents/' directory.\")\n",
    "        faiss_index = None\n",
    "    \n",
    "    INDEX_INITIALIZED = True\n",
    "    return faiss_index\n",
    "\n",
    "\n",
    "def load_prompt_config(file_name):\n",
    "    # Load configuration\n",
    "    try:\n",
    "        with open(file_name, 'r', encoding='utf-8') as config_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "        return config\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"prompt.yaml not found\")\n",
    "        raise\n",
    "    except yaml.YAMLError:\n",
    "        logger.error(\"Invalid YAML in prompt.yaml\")\n",
    "        raise\n",
    "\n",
    "def create_prompt(prompt_template, passages, query, max_context_tokens=3000):\n",
    "    total_length = len(query) + len(prompt_template) - len(\"{passages}\") - len(\"{query}\")\n",
    "    passages_text = \"\"\n",
    "    passage_refs = []\n",
    "    for idx, passage in enumerate(passages, 1):\n",
    "        # passage = ''.join([i.strip() for i in '\\n'.split(passage)])\n",
    "        ref_id = f\"[Ref{idx}]\"\n",
    "        if total_length + len(passage) < max_context_tokens * 4:\n",
    "            passages_text += f\"{ref_id} {passage}\\n\"\n",
    "            passage_refs.append((ref_id, passage))\n",
    "            total_length += len(passage) + 1\n",
    "        else:\n",
    "            logger.warning(f\"Truncated passages to fit within {max_context_tokens} tokens\")\n",
    "            break\n",
    "    logger.info(f\"Passages provided in prompt: {passages_text}\")\n",
    "    return prompt_template.format(passages=passages_text, query=query), passage_refs\n",
    "\n",
    "# Function to cluster passages based on semantic similarity\n",
    "def cluster_passages(passages, embeddings, max_clusters=3):\n",
    "    if len(passages) <= 1:\n",
    "        return [(passages, embeddings)] if passages else []\n",
    "    \n",
    "    # Use KMeans to cluster embeddings\n",
    "    num_clusters = min(max_clusters, len(passages))\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    # Group passages by cluster\n",
    "    clustered_passages = [[] for _ in range(num_clusters)]\n",
    "    clustered_embeddings = [[] for _ in range(num_clusters)]\n",
    "    for idx, label in enumerate(labels):\n",
    "        clustered_passages[label].append(passages[idx])\n",
    "        clustered_embeddings[label].append(embeddings[idx])\n",
    "    \n",
    "    # Return clusters as list of (passages, embeddings) tuples\n",
    "    return [(clustered_passages[i], clustered_embeddings[i]) for i in range(num_clusters) if clustered_passages[i]]\n",
    "\n",
    "# Function to compute semantic similarity between two texts\n",
    "def compute_similarity(text1, text2):\n",
    "    embeddings = embedder.encode([text1, text2], show_progress_bar=False)\n",
    "    similarity = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))\n",
    "    return similarity\n",
    "\n",
    "# Function to format the response as HTML with citations at the end\n",
    "def format_response(bot_message, passage_refs, merged_chunks):\n",
    "    logger.info(f\"Bot message: {bot_message}\")\n",
    "    \n",
    "    # Convert the bot message to HTML, preserving paragraphs and bullet points\n",
    "    lines = bot_message.strip().split('\\n')\n",
    "    formatted_lines = []\n",
    "    in_list = False\n",
    "    used_refs = set()\n",
    "    \n",
    "    # Parse the answer to find referenced passages\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Skip the \"References:\" line at the end\n",
    "        if line.startswith(\"References:\"):\n",
    "            continue\n",
    "        # Look for [RefX] patterns in the line\n",
    "        refs_in_line = re.findall(r'\\[Ref\\d+\\]', line)\n",
    "        used_refs.update(refs_in_line)\n",
    "        # Remove [RefX] from the line for display\n",
    "        line = re.sub(r'\\[Ref\\d+\\]', '', line).strip()\n",
    "        if not line:\n",
    "            if in_list:\n",
    "                formatted_lines.append('</ul>')\n",
    "                in_list = False\n",
    "            continue\n",
    "        if line.startswith('- '):\n",
    "            if not in_list:\n",
    "                formatted_lines.append('<ul>')\n",
    "                in_list = True\n",
    "            formatted_lines.append(f'<li>{line[2:]}</li>')\n",
    "        else:\n",
    "            if in_list:\n",
    "                formatted_lines.append('</ul>')\n",
    "                in_list = False\n",
    "            formatted_lines.append(f'<p>{line}</p>')\n",
    "    \n",
    "    if in_list:\n",
    "        formatted_lines.append('</ul>')\n",
    "    \n",
    "    # Generate citations for used references\n",
    "    citations = []\n",
    "    if used_refs:\n",
    "        for ref_id, passage in passage_refs:\n",
    "            if ref_id in used_refs:\n",
    "                filepath = passage.split('\\n')[0].replace(\"Document: \", \"\")\n",
    "                for chunk in merged_chunks:\n",
    "                    if chunk['filepath'] == filepath:\n",
    "                        citation = f\"Source: {os.path.basename(chunk['filepath'])}\"\n",
    "                        if chunk['pages']:\n",
    "                            pages_str = ', '.join(map(str, sorted(set(chunk['pages']))))\n",
    "                            citation += f\", Pages/Slides: {pages_str}\"\n",
    "                        if citation not in citations:\n",
    "                            citations.append(citation)\n",
    "    else:\n",
    "        # Fallback: Use semantic similarity to determine relevant passages\n",
    "        logger.warning(\"No references explicitly used in the answer. Using semantic similarity to find relevant passages.\")\n",
    "        answer_text = bot_message.lower()\n",
    "        for ref_id, passage in passage_refs:\n",
    "            passage_text = passage.lower()\n",
    "            # Check for keyword overlap\n",
    "            answer_words = set(answer_text.split())\n",
    "            passage_words = set(passage_text.split())\n",
    "            common_words = answer_words.intersection(passage_words)\n",
    "            # Compute semantic similarity\n",
    "            similarity = compute_similarity(answer_text, passage_text)\n",
    "            # Include the passage if there is significant overlap or high similarity\n",
    "            if len(common_words) > 3 or similarity > 0.7:\n",
    "                used_refs.add(ref_id)\n",
    "                filepath = passage.split('\\n')[0].replace(\"Document: \", \"\")\n",
    "                for chunk in merged_chunks:\n",
    "                    if chunk['filepath'] == filepath:\n",
    "                        citation = f\"Source: {os.path.basename(chunk['filepath'])}\"\n",
    "                        if chunk['pages']:\n",
    "                            pages_str = ', '.join(map(str, sorted(set(chunk['pages']))))\n",
    "                            citation += f\", Pages/Slides: {pages_str}\"\n",
    "                        if citation not in citations:\n",
    "                            citations.append(citation)\n",
    "        if not citations:\n",
    "            logger.warning(\"No relevant passages found via semantic similarity. No citations will be included.\")\n",
    "\n",
    "    # Add citations at the end\n",
    "    if citations:\n",
    "        citations_html = '<div class=\"references\"><strong>References:</strong><br>' + '<br>'.join(citations) + '</div>'\n",
    "        formatted_lines.append(citations_html)\n",
    "    \n",
    "    return ''.join(formatted_lines)\n",
    "\n",
    "def prepare_passages(user_message, faiss_index):\n",
    "    query_embedding = embedder.encode([user_message], show_progress_bar=False)\n",
    "    query_embedding = np.array(query_embedding).astype('float32')\n",
    "    if len(query_embedding.shape) == 1:\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "    logger.info(f\"Query embedding generated - shape: {query_embedding.shape}, norm: {np.linalg.norm(query_embedding)}\")\n",
    "    passages = []\n",
    "    passage_embeddings = []\n",
    "    merged_chunks = []\n",
    "    clusters = []\n",
    "    if faiss_index is not None and len(documents) > 0:\n",
    "        logger.info('='*10 + \" Performing FAISS search\" + \"=\"*10)\n",
    "        distances, indices = faiss_index.search(query_embedding, k=num_retrieved_indices)\n",
    "        logger.info(f\"FAISS search results:\")\n",
    "        logger.info(f\"- Retrieved indices: {indices[0].tolist()}\")\n",
    "        logger.info(f\"- Distances: {distances[0].tolist()}\")\n",
    "        logger.info(distances[0])\n",
    "        logger.info('='*30)\n",
    "        valid_indices = [idx for idx in indices[0] if 0 <= idx < len(chunk_metadata)]\n",
    "        if not valid_indices:\n",
    "            logger.warning(\"No valid indices retrieved from FAISS search\")\n",
    "            passages = []\n",
    "            passage_embeddings = []\n",
    "        else:\n",
    "            valid_indices.sort(key=lambda idx: (chunk_metadata[idx]['filepath'], chunk_metadata[idx]['chunk_index']))\n",
    "            \n",
    "            current_chunk = None\n",
    "            for idx in valid_indices:\n",
    "                chunk_info = chunk_metadata[idx]\n",
    "                if (current_chunk is None or\n",
    "                        current_chunk['filepath'] != chunk_info['filepath'] or\n",
    "                        current_chunk['chunk_index'] + 1 != chunk_info['chunk_index']):\n",
    "                    if current_chunk is not None:\n",
    "                        merged_chunks.append(current_chunk)\n",
    "                    current_chunk = {\n",
    "                        'filepath': chunk_info['filepath'],\n",
    "                        'chunk_index': chunk_info['chunk_index'],\n",
    "                        'text': chunk_info['original_text'],\n",
    "                        'pages': [chunk_info['page']] if chunk_info['page'] is not None else []\n",
    "                    }\n",
    "                else:\n",
    "                    current_chunk['text'] += \" \" + chunk_info['original_text']\n",
    "                    current_chunk['chunk_index'] = chunk_info['chunk_index']\n",
    "                    if chunk_info['page'] is not None:\n",
    "                        current_chunk['pages'].append(chunk_info['page'])\n",
    "            if current_chunk is not None:\n",
    "                merged_chunks.append(current_chunk)\n",
    "            \n",
    "            # Create passages and embeddings for clustering\n",
    "            for chunk in merged_chunks:\n",
    "                sep_sign = '--'*15\n",
    "                passage = f\"Document: {chunk['filepath']}\\nContent: {chunk['text']}\\n{sep_sign}\\n\"\n",
    "                passages.append(passage)\n",
    "                passage_embeddings.append(embedder.encode([passage], show_progress_bar=False)[0])\n",
    "            \n",
    "            # Cluster passages based on semantic similarity\n",
    "            passage_embeddings = np.array(passage_embeddings).astype('float32')\n",
    "            clusters = cluster_passages(passages, passage_embeddings)\n",
    "            \n",
    "            # Rebuild passages based on clusters\n",
    "            passages = []\n",
    "            for clustered_passages, _ in clusters:\n",
    "                cluster_text = \"\\n\\n\".join(clustered_passages)\n",
    "                passages.append(cluster_text)\n",
    "            \n",
    "            logger.info(f\"Clustered passages: {passages}\")\n",
    "            return passages, merged_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting document indexing...\n",
      "INFO:__main__:process for LLD Dacomitinib - CHN - Chinese (Simplified).PDF\n",
      "INFO:__main__:process for ./documents/LLD Dacomitinib - CHN - Chinese (Simplified).PDF\n",
      "INFO:__main__:Attempting to read PDF: ./documents/LLD Dacomitinib - CHN - Chinese (Simplified).PDF\n",
      "INFO:__main__:PDF file loaded.\n",
      "INFO:__main__:==========Created 1 chunks from 616 characters==========\n",
      "INFO:__main__:Chunk 0:  617 chars | Start: 第1页，共12页\n",
      "Version No. : 20240229\n",
      "核准日期：2019 年05 月15 日\n",
      "修改日期：2020 年10 月16 日；2021 年 08 月09 日；2024 年 02 月29 日\n",
      "达可替尼片说明书\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:==========Created 1 chunks from 900 characters==========\n",
      "INFO:__main__:Chunk 0:  901 chars | Start: 第2页，共12页\n",
      "Version No. : 20240229\n",
      "本品的推荐剂量为每日一次口服 45 mg，直至出现疾病进展或不可接受的毒性。本品可与食物同服，\n",
      "也可不与食物同服（见【药代动力学】）。\n",
      "每天在大致相同的时间服用本品。如果患者呕\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:==========Created 2 chunks from 1434 characters==========\n",
      "INFO:__main__:Chunk 0: 1000 chars | Start: 第3页，共12页\n",
      "Version No. : 20240229\n",
      "不建议 对轻度或中度肾功能损害（ 依据 Cockcroft-Gault 公式预计肌酐清 除率 [CLcr] 在 30~89\n",
      "mL/min）的患者调整剂量。尚未确定重度肾功能损害\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:Chunk 1:  435 chars | Start: ≥ 1%）的严重不良反应有腹泻（2.2%）、\n",
      "间质性肺病（1.3%）。57% 接受本品治疗的患者曾中断给药。导致给药中断的最常见（发生率 > 5%）\n",
      "不良反应有皮疹（23%）、甲沟炎（13%）和腹泻（10%）。66% 接受本品治疗的患者曾降\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:==========Created 1 chunks from 857 characters==========\n",
      "INFO:__main__:Chunk 0:  858 chars | Start: 第4页，共12页\n",
      "Version No. : 20240229\n",
      "不良反应 达可替尼\n",
      "（N = 227）\n",
      "吉非替尼\n",
      "（N = 224）\n",
      "所有级别 a\n",
      "%\n",
      "3 和4 级\n",
      "%\n",
      "所有级别\n",
      "%\n",
      "3 和4 级\n",
      "%\n",
      "胃肠系统异常\n",
      "腹泻 b\n",
      "口腔黏膜炎 c\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:==========Created 1 chunks from 913 characters==========\n",
      "INFO:__main__:Chunk 0:  914 chars | Start: 第5页，共12页\n",
      "Version No. : 20240229\n",
      "全身性异常：疲乏 9%\n",
      "皮肤和皮下组织异常：皮肤皲裂 9%、多毛 1.3%、皮肤剥脱/剥脱性皮肤反应 3.5%\n",
      "胃肠系统异常：呕吐 9%\n",
      "神经系统异常：味觉障碍 7%\n",
      "呼吸系统\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:==========Created 1 chunks from 1277 characters==========\n",
      "INFO:__main__:Chunk 0: 1000 chars | Start: 第6页，共12页\n",
      "Version No. : 20240229\n",
      "86%；其中，11% 的患者报告了 3 或 4 级腹泻，0.3% 的病例致死。\n",
      "对于 ≥ 2 级腹泻，请暂时停用本品，直到恢复至 ≤ 1 级，然后根据腹泻严重程度，按相同剂量水\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:Chunk 1:  278 chars | Start: 风险。\n",
      "数据\n",
      "动物数据\n",
      "器官形成期间，妊娠大鼠每日口服 5 mg/kg/天（约为建议人用剂量时暴露量（基于曲线下面积 [AUC] \n",
      "的 1.2 倍）达可替尼后，导致着床后流产、母体毒性以及胎儿体重下降的发生率增加。\n",
      "小鼠模型中的 EGFR\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:==========Created 1 chunks from 883 characters==========\n",
      "INFO:__main__:Chunk 0:  884 chars | Start: 第7页，共12页\n",
      "Version No. : 20240229\n",
      "风险总结\n",
      "目前尚无有关人乳中是否存在达可替尼或其代谢物或他们对母乳喂养婴儿或乳汁产生的影响的信息。\n",
      "由于母乳喂养的婴儿可能出现因本品导致的严重不良反应，因此应告知女性在使用本品\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:==========Created 2 chunks from 1678 characters==========\n",
      "INFO:__main__:Chunk 0: 1000 chars | Start: 第8页，共12页\n",
      "Version No. : 20240229\n",
      "本品的疗效已在一项随机、国际多中心、开放性研究（ARCHER 1050）中得到证实。该项研究要求患\n",
      "者为无法切除的局部晚期或转移性 NSCLC，既往无治疗史，或为完成全身治疗后\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:Chunk 1:   10 chars | Start: 参见表5。\n",
      "表 5.\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:Chunk 2:  668 chars | Start: ARCHER 1050 研究中的疗效结果*\n",
      "达可替尼\n",
      "N = 227\n",
      "吉非替尼\n",
      "N = 225\n",
      "无进展生存期（基于IRC 审评）\n",
      "出现事件的患者人数，n（%） 136 (59.9%) 179 (79.6%)\n",
      "中位无进展生存期，月（95% C\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:==========Created 1 chunks from 266 characters==========\n",
      "INFO:__main__:Chunk 0:  267 chars | Start: 第9页，共12页\n",
      "Version No. : 20240229\n",
      "图 1. ARCHER 1050 研究中基于 IRC 审评的 PFS 的 Kaplan-Meier 曲线\n",
      "图 2. ARCHER 1050 研究中 OS 的 Kaplan-Me\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:==========Created 1 chunks from 1432 characters==========\n",
      "INFO:__main__:Chunk 0: 1000 chars | Start: 第10页，共12页\n",
      "Version No. : 20240229\n",
      "【药理毒理】\n",
      "药理作用\n",
      "达可替尼是人表皮生长因子受体家族（EGFR/HER1、HER2 和 HER4）和某些EGFR 激活突变体（19 \n",
      "号外显子缺失或21 号外显子 L85\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:Chunk 1:  433 chars | Start: x 为108 ng/mL（35%），AUC0-24h 为2213 ng•h/mL（35%）。在重复给药后14 天内达到稳\n",
      "态，基于AUC 估计的几何平均（CV%）蓄积比为5.7（28%）。\n",
      "吸收\n",
      "口服给药后，达可替尼的平均绝对生物利用度为8\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:==========Created 1 chunks from 1309 characters==========\n",
      "INFO:__main__:Chunk 0: 1000 chars | Start: 第11页，共12页\n",
      "Version No. : 20240229\n",
      "的几何平均（CV%）表观血浆清除率为24.9 L/h（36%）。\n",
      "代谢\n",
      "肝脏代谢是达可替尼的主要清除途径，氧化作用和谷胱甘肽结合为主要代谢反应。在单次口服 45 mg \n",
      "[1\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:Chunk 1:  310 chars | Start: 尼及其活性代谢物（O-去甲基达可替尼）在血浆中的总 AUClast 增加约 6%，不认为该结果与临\n",
      "床相关。\n",
      "达可替尼对 CYP2D6 底物的影响\n",
      "与本品45 mg 单次口服给药合用，使右美沙芬（一种CYP2D6 底物）的Cmax 和AUC\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:==========Created 1 chunks from 483 characters==========\n",
      "INFO:__main__:Chunk 0:  484 chars | Start: 第12页，共12页\n",
      "Version No. : 20240229\n",
      "【贮藏】\n",
      "30℃以下保存。\n",
      "【包装】\n",
      "（1）高密度聚乙烯瓶装：30 片/瓶。\n",
      "（2）铝/铝泡罩包装：10 片/盒，30 片/盒。\n",
      "【有效期】\n",
      "60 个月。\n",
      "【执行标准】\n",
      "进口\n",
      "INFO:__main__:----------------------------------------\n",
      "INFO:__main__:Successfully read 18 chunks from ./documents/LLD Dacomitinib - CHN - Chinese (Simplified).PDF\n",
      "INFO:__main__:total pages extracted: 18\n",
      "INFO:__main__:Generating embeddings for 18 document chunks\n",
      "INFO:__main__:Sample document chunk: 第1页，共12页\n",
      "Version No. : 20240229\n",
      "核准日期：2019 年05 月15 日\n",
      "修改日期：2020 年10 月16 日；2021 年 08 月09 日；2024 年 02 月2...\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "INFO:__main__:Generated embeddings - shape: (18, 768), dtype: float32\n",
      "INFO:__main__:Creating FAISS index with dimension 768\n",
      "INFO:__main__:Indexed 18 document chunks\n",
      "INFO:__main__:FAISS index size: 18\n"
     ]
    }
   ],
   "source": [
    "faiss_index = initialize_index()\n",
    "prompt_config = load_prompt_config('prompt.yaml')\n",
    "SYSTEM_PROMPT = prompt_config.get('system_prompt', '')\n",
    "prompt_template = prompt_config.get('prompt_template', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Query embedding generated - shape: (1, 768), norm: 5.977413654327393\n",
      "INFO:__main__:========== Performing FAISS search==========\n",
      "INFO:__main__:FAISS search results:\n",
      "INFO:__main__:- Retrieved indices: [7, 16, 0, 14, 1]\n",
      "INFO:__main__:- Distances: [30.50945281982422, 32.47759246826172, 33.56614685058594, 33.96271896362305, 34.227542877197266]\n",
      "INFO:__main__:[30.509453 32.477592 33.566147 33.96272  34.227543]\n",
      "INFO:__main__:==============================\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "INFO:__main__:Clustered passages: ['Document: ./documents/LLD Dacomitinib - CHN - Chinese (Simplified).PDF\\nContent: 风险。\\n数据\\n动物数据\\n器官形成期间，妊娠大鼠每日口服 5 mg/kg/天（约为建议人用剂量时暴露量（基于曲线下面积 [AUC] \\n的 1.2 倍）达可替尼后，导致着床后流产、母体毒性以及胎儿体重下降的发生率增加。\\n小鼠模型中的 EGFR 被破坏或消耗，表明 EGFR 在生殖和发育过程（包括胚泡植入、胎盘发育和胚\\n胎-胎儿/出生后存活和发育）中至关重要。小鼠胚胎-胎儿或母体 EGFR 信号传导的降低或消除会阻碍\\n植入，而且会在妊娠的各个阶段（通过对胎盘发育的影响）导致胚胎-胎儿流产、发育异常、活胎早亡\\n以及胚胎/新生儿的多个器官发育不良。\\n哺乳期妇女\\n------------------------------\\n\\n\\nDocument: ./documents/LLD Dacomitinib - CHN - Chinese (Simplified).PDF\\nContent: x 为108 ng/mL（35%），AUC0-24h 为2213 ng•h/mL（35%）。在重复给药后14 天内达到稳\\n态，基于AUC 估计的几何平均（CV%）蓄积比为5.7（28%）。\\n吸收\\n口服给药后，达可替尼的平均绝对生物利用度为80%。在癌症患者中，单次口服本品45 mg 后，达可\\n替尼达到最大浓度的中位时间（Tmax）约为 6.0 小时（范围2.0 至24 小时）。随高脂肪、高热量膳食（大\\n约 800-1000 卡路里；蛋白质、碳水化合物和脂肪分别提供150、250 和500-600 卡路里）一并服用本品\\n对达可替尼药代动力学没有临床意义上的影响。\\n分布\\n达可替尼的几何平均（CV%）分布容积（Vss）为 1889 L（18%）。达可替尼与人血浆蛋白的体外结合\\n率约为98%，不依赖药物浓度（250 ng/mL 至1000 ng/mL）。\\n消除\\n癌症患者单次口服45 mg 本品后，达可替尼的平均（CV%）血浆半衰期为70 小时（21%），达可替尼\\n------------------------------\\n', 'Document: ./documents/LLD Dacomitinib - CHN - Chinese (Simplified).PDF\\nContent: 尼及其活性代谢物（O-去甲基达可替尼）在血浆中的总 AUClast 增加约 6%，不认为该结果与临\\n床相关。\\n达可替尼对 CYP2D6 底物的影响\\n与本品45 mg 单次口服给药合用，使右美沙芬（一种CYP2D6 底物）的Cmax 和AUClast 分别升高9.7 倍\\n和 9.6 倍（见【药物相互作用】）。\\n本品在一项I 期研究（A7471051）中评估了14 例中国健康受试者单次空腹口服本品45mg 后的药代动\\n力学特征，并在一项国际多中心研究（ARCHER 1050）中评估了19 例中国患者多次口服本品45mg 后\\n的药代动力学特征。上述中国人群的药代动力学结果中，未观察到与非中国人群存在明显的暴露量差\\n异。\\n------------------------------\\n', 'Document: ./documents/LLD Dacomitinib - CHN - Chinese (Simplified).PDF\\nContent: 第1页，共12页\\nVersion No. : 20240229\\n核准日期：2019 年05 月15 日\\n修改日期：2020 年10 月16 日；2021 年 08 月09 日；2024 年 02 月29 日\\n达可替尼片说明书\\n                   \\n  请仔细阅读说明书并在医师指导下使用\\n【药品名称】\\n通用名称：达可替尼片\\n商品名称：多泽润®/VIZIMPRO®\\n英文名称：Dacomitinib Tablets\\n汉语拼音：Daketini Pian\\n【成份】\\n本品活性成份为达可替尼\\n其化学名称为：\\n(2E)-N-{4-[(3-氯-4-氟苯基)氨基]-7-甲氧基喹唑啉-6-基}-4-(哌啶-1-基)丁-2-一水烯胺\\n化学结构式：\\n分子式：C24H25ClFN5O2 · H2O\\n分子量：487.95 \\n【性状】\\n本品为蓝色薄膜衣片，除去包衣后显白色或类白色。\\n【适应症】\\n单药用于表皮生长因子受体（EGFR）19 号外显子缺失突变或21 号外显子L858R 置换突变的局部晚期\\n或转移性非小细胞肺癌（NSCLC）患者的一线治疗。\\n【规格】\\n(1) 15 mg; (2) 45 mg\\n【用法用量】\\n患者选择\\n本品应在有使用经验的医疗机构中并在特定的专业技术人员指导下使用。必须基于肿瘤样本经充分验\\n证的检测方法证实为EGFR 19 号外显子缺失突变或21 号外显子L858R 置换突变阳性的患者方可使用。\\n推荐剂量 第2页，共12页\\nVersion No. : 20240229\\n本品的推荐剂量为每日一次口服 45 mg，直至出现疾病进展或不可接受的毒性。本品可与食物同服，\\n也可不与食物同服（见【药代动力学】）。\\n每天在大致相同的时间服用本品。如果患者呕吐或漏服一剂，不应追加剂量或补充服用漏服剂量，而\\n应在下一次的服药时间服用规定剂量。\\n针对不良反应的剂量调整\\n如果出现不良反应，按照表 1 中所述降低本品的剂量。表 2 提供了针对特定不良反应的剂量调整。\\n表 1. 针对不良反应的本品推荐剂量降低方案\\n剂量水平 剂量（每日一次）\\n第一次剂量降低 30 mg\\n第二次剂量降低 15 mg\\n表 2. 针对不良反应的本品剂量调整\\n不良反应 严重程度\\na\\n剂量调整\\n间质性肺病（ILD）（见\\n【注意事项】）\\n所有级别 \\uf0b7 永久停用本品。\\n腹泻（见【注意事项】）2 级 \\uf0b7 暂时停用本品，直到恢复至 ≤ 1 级；然后按相同剂量水平\\n继续服用本品。\\n\\uf0b7 对于再次出现的 2 级腹泻，暂时停用本品，直到恢复至 ≤ 1 \\n级；降低一个剂量水平继续服用本品。\\n3 或4 级 \\uf0b7 暂时停用本品，直到恢复至 ≤ 1 级；然后降低一个剂量水\\n平继续服用本品。\\n皮肤不良反应 （见\\n【注意事项】）\\n2 级 \\uf0b7 对于持续性皮肤不良反应，暂时停用本品。一旦恢复至 ≤ 1 \\n级，按相同剂量水平继续服用本品。\\n\\uf0b7 对于再次出现的持续性 2 级皮肤不良反应，暂时停用本\\n品，直到恢复至 ≤ 1 级，然后降低一个剂量水平继续服用\\n本品。\\n3 或4 级 \\uf0b7 暂时停用本品，直到恢复至 ≤ 1 级；然后降低一个剂量水\\n平继续服用本品。\\n其它 3 或4 级 \\uf0b7 暂时停用本品，直到恢复至 ≤ 2 级；然后降低一个剂量水\\n平继续服用本品。\\na 美国国立癌症研究院通用不良事件术语标准第4.03 版\\n针对使用降酸剂的剂量调整\\n服用本品时，避免同时使用质子泵抑制剂（PPI）。可使用局部作用的抗酸剂或组胺 2（H2）受体拮抗\\n剂代替 PPI；在服用 H2 受体拮抗剂至少 6 小时前或至少 10 小时后给予本品（见【药物相互作用】和\\n【药代动力学】）。\\n肾损害\\n------------------------------\\n']\n",
      "INFO:__main__:Constructing RAG prompt...\n",
      "INFO:__main__:Passages provided in prompt: [Ref1] Document: ./documents/LLD Dacomitinib - CHN - Chinese (Simplified).PDF\n",
      "Content: 风险。\n",
      "数据\n",
      "动物数据\n",
      "器官形成期间，妊娠大鼠每日口服 5 mg/kg/天（约为建议人用剂量时暴露量（基于曲线下面积 [AUC] \n",
      "的 1.2 倍）达可替尼后，导致着床后流产、母体毒性以及胎儿体重下降的发生率增加。\n",
      "小鼠模型中的 EGFR 被破坏或消耗，表明 EGFR 在生殖和发育过程（包括胚泡植入、胎盘发育和胚\n",
      "胎-胎儿/出生后存活和发育）中至关重要。小鼠胚胎-胎儿或母体 EGFR 信号传导的降低或消除会阻碍\n",
      "植入，而且会在妊娠的各个阶段（通过对胎盘发育的影响）导致胚胎-胎儿流产、发育异常、活胎早亡\n",
      "以及胚胎/新生儿的多个器官发育不良。\n",
      "哺乳期妇女\n",
      "------------------------------\n",
      "\n",
      "\n",
      "Document: ./documents/LLD Dacomitinib - CHN - Chinese (Simplified).PDF\n",
      "Content: x 为108 ng/mL（35%），AUC0-24h 为2213 ng•h/mL（35%）。在重复给药后14 天内达到稳\n",
      "态，基于AUC 估计的几何平均（CV%）蓄积比为5.7（28%）。\n",
      "吸收\n",
      "口服给药后，达可替尼的平均绝对生物利用度为80%。在癌症患者中，单次口服本品45 mg 后，达可\n",
      "替尼达到最大浓度的中位时间（Tmax）约为 6.0 小时（范围2.0 至24 小时）。随高脂肪、高热量膳食（大\n",
      "约 800-1000 卡路里；蛋白质、碳水化合物和脂肪分别提供150、250 和500-600 卡路里）一并服用本品\n",
      "对达可替尼药代动力学没有临床意义上的影响。\n",
      "分布\n",
      "达可替尼的几何平均（CV%）分布容积（Vss）为 1889 L（18%）。达可替尼与人血浆蛋白的体外结合\n",
      "率约为98%，不依赖药物浓度（250 ng/mL 至1000 ng/mL）。\n",
      "消除\n",
      "癌症患者单次口服45 mg 本品后，达可替尼的平均（CV%）血浆半衰期为70 小时（21%），达可替尼\n",
      "------------------------------\n",
      "\n",
      "[Ref2] Document: ./documents/LLD Dacomitinib - CHN - Chinese (Simplified).PDF\n",
      "Content: 尼及其活性代谢物（O-去甲基达可替尼）在血浆中的总 AUClast 增加约 6%，不认为该结果与临\n",
      "床相关。\n",
      "达可替尼对 CYP2D6 底物的影响\n",
      "与本品45 mg 单次口服给药合用，使右美沙芬（一种CYP2D6 底物）的Cmax 和AUClast 分别升高9.7 倍\n",
      "和 9.6 倍（见【药物相互作用】）。\n",
      "本品在一项I 期研究（A7471051）中评估了14 例中国健康受试者单次空腹口服本品45mg 后的药代动\n",
      "力学特征，并在一项国际多中心研究（ARCHER 1050）中评估了19 例中国患者多次口服本品45mg 后\n",
      "的药代动力学特征。上述中国人群的药代动力学结果中，未观察到与非中国人群存在明显的暴露量差\n",
      "异。\n",
      "------------------------------\n",
      "\n",
      "[Ref3] Document: ./documents/LLD Dacomitinib - CHN - Chinese (Simplified).PDF\n",
      "Content: 第1页，共12页\n",
      "Version No. : 20240229\n",
      "核准日期：2019 年05 月15 日\n",
      "修改日期：2020 年10 月16 日；2021 年 08 月09 日；2024 年 02 月29 日\n",
      "达可替尼片说明书\n",
      "                   \n",
      "  请仔细阅读说明书并在医师指导下使用\n",
      "【药品名称】\n",
      "通用名称：达可替尼片\n",
      "商品名称：多泽润®/VIZIMPRO®\n",
      "英文名称：Dacomitinib Tablets\n",
      "汉语拼音：Daketini Pian\n",
      "【成份】\n",
      "本品活性成份为达可替尼\n",
      "其化学名称为：\n",
      "(2E)-N-{4-[(3-氯-4-氟苯基)氨基]-7-甲氧基喹唑啉-6-基}-4-(哌啶-1-基)丁-2-一水烯胺\n",
      "化学结构式：\n",
      "分子式：C24H25ClFN5O2 · H2O\n",
      "分子量：487.95 \n",
      "【性状】\n",
      "本品为蓝色薄膜衣片，除去包衣后显白色或类白色。\n",
      "【适应症】\n",
      "单药用于表皮生长因子受体（EGFR）19 号外显子缺失突变或21 号外显子L858R 置换突变的局部晚期\n",
      "或转移性非小细胞肺癌（NSCLC）患者的一线治疗。\n",
      "【规格】\n",
      "(1) 15 mg; (2) 45 mg\n",
      "【用法用量】\n",
      "患者选择\n",
      "本品应在有使用经验的医疗机构中并在特定的专业技术人员指导下使用。必须基于肿瘤样本经充分验\n",
      "证的检测方法证实为EGFR 19 号外显子缺失突变或21 号外显子L858R 置换突变阳性的患者方可使用。\n",
      "推荐剂量 第2页，共12页\n",
      "Version No. : 20240229\n",
      "本品的推荐剂量为每日一次口服 45 mg，直至出现疾病进展或不可接受的毒性。本品可与食物同服，\n",
      "也可不与食物同服（见【药代动力学】）。\n",
      "每天在大致相同的时间服用本品。如果患者呕吐或漏服一剂，不应追加剂量或补充服用漏服剂量，而\n",
      "应在下一次的服药时间服用规定剂量。\n",
      "针对不良反应的剂量调整\n",
      "如果出现不良反应，按照表 1 中所述降低本品的剂量。表 2 提供了针对特定不良反应的剂量调整。\n",
      "表 1. 针对不良反应的本品推荐剂量降低方案\n",
      "剂量水平 剂量（每日一次）\n",
      "第一次剂量降低 30 mg\n",
      "第二次剂量降低 15 mg\n",
      "表 2. 针对不良反应的本品剂量调整\n",
      "不良反应 严重程度\n",
      "a\n",
      "剂量调整\n",
      "间质性肺病（ILD）（见\n",
      "【注意事项】）\n",
      "所有级别  永久停用本品。\n",
      "腹泻（见【注意事项】）2 级  暂时停用本品，直到恢复至 ≤ 1 级；然后按相同剂量水平\n",
      "继续服用本品。\n",
      " 对于再次出现的 2 级腹泻，暂时停用本品，直到恢复至 ≤ 1 \n",
      "级；降低一个剂量水平继续服用本品。\n",
      "3 或4 级  暂时停用本品，直到恢复至 ≤ 1 级；然后降低一个剂量水\n",
      "平继续服用本品。\n",
      "皮肤不良反应 （见\n",
      "【注意事项】）\n",
      "2 级  对于持续性皮肤不良反应，暂时停用本品。一旦恢复至 ≤ 1 \n",
      "级，按相同剂量水平继续服用本品。\n",
      " 对于再次出现的持续性 2 级皮肤不良反应，暂时停用本\n",
      "品，直到恢复至 ≤ 1 级，然后降低一个剂量水平继续服用\n",
      "本品。\n",
      "3 或4 级  暂时停用本品，直到恢复至 ≤ 1 级；然后降低一个剂量水\n",
      "平继续服用本品。\n",
      "其它 3 或4 级  暂时停用本品，直到恢复至 ≤ 2 级；然后降低一个剂量水\n",
      "平继续服用本品。\n",
      "a 美国国立癌症研究院通用不良事件术语标准第4.03 版\n",
      "针对使用降酸剂的剂量调整\n",
      "服用本品时，避免同时使用质子泵抑制剂（PPI）。可使用局部作用的抗酸剂或组胺 2（H2）受体拮抗\n",
      "剂代替 PPI；在服用 H2 受体拮抗剂至少 6 小时前或至少 10 小时后给予本品（见【药物相互作用】和\n",
      "【药代动力学】）。\n",
      "肾损害\n",
      "------------------------------\n",
      "\n",
      "\n",
      "INFO:__main__:Final prompt length: 3794 characters\n"
     ]
    }
   ],
   "source": [
    "user_message = '达可替尼片的分子式和英文名是什么'\n",
    "# user_message = '根据达可替尼片的分子式,在当前domain,有哪些类似的药品'\n",
    "# user_message = '根据达可替尼片的特性和副作用，我们目前有哪些相似的药品'\n",
    "# user_message = '在当前正在实验的新药中，我们目前有哪些相似的药品和达可替尼片的特性相近，并且副作用要小'\n",
    "\n",
    "passages, merged_chunks = prepare_passages(user_message,faiss_index)\n",
    "\n",
    "logger.info(\"Constructing RAG prompt...\")\n",
    "prompt, passage_refs = create_prompt(prompt_template, passages, user_message, max_context_tokens=3000)\n",
    "logger.info(f\"Final prompt length: {len(prompt)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base your answer strictly on the information provided in the following internal project documents. Structure your answer as follows:\n",
      "1. Start with a brief summary paragraph that provides an overview of the answer.\n",
      "2. Follow with detailed information in a natural format. Use bullet points only when listing multiple items (e.g., information about multiple projects). Otherwise, use natural paragraphs.\n",
      "3. For every piece of information you use from the documents, you MUST include the corresponding Reference ID (e.g., [Ref1], [Ref2]) inline with the text. If you do not use any document, explicitly state that no documents were used.\n",
      "4. At the end of your answer, list the Reference IDs you used (e.g., References: [Ref1], [Ref2]).\n",
      "\n",
      "Documents:\n",
      "[Ref1] Document: ./documents/LLD Dacomitinib - CHN - Chinese (Simplified).PDF\n",
      "Content: 风险。\n",
      "数据\n",
      "动物数据\n",
      "器官形成期间，妊娠大鼠每日口服 5 mg/kg/天（约为建议人用剂量时暴露量（基于曲线下面积 [AUC] \n",
      "的 1.2 倍）达可替尼后，导致着床后流产、母体毒性以及胎儿体重下降的发生率增加。\n",
      "小鼠模型中的 EGFR 被破坏或消耗，表明 EGFR 在生殖和发育过程（包括胚泡植入、胎盘发育和胚\n",
      "胎-胎儿/出生后存活和发育）中至关重要。小鼠胚胎-胎儿或母体 EGFR 信号传导的降低或消除会阻碍\n",
      "植入，而且会在妊娠的各个阶段（通过对胎盘发育的影响）导致胚胎-胎儿流产、发育异常、活胎早亡\n",
      "以及胚胎/新生儿的多个器官发育不良。\n",
      "哺乳期妇女\n",
      "------------------------------\n",
      "\n",
      "\n",
      "Document: ./documents/LLD Dacomitinib - CHN - Chinese (Simplified).PDF\n",
      "Content: x 为108 ng/mL（35%），AUC0-24h 为2213 ng•h/mL（35%）。在重复给药后14 天内达到稳\n",
      "态，基于AUC 估计的几何平均（CV%）蓄积比为5.7（28%）。\n",
      "吸收\n",
      "口服给药后，达可替尼的平均绝对生物利用度为80%。在癌症患者中，单次口服本品45 mg 后，达可\n",
      "替尼达到最大浓度的中位时间（Tmax）约为 6.0 小时（范围2.0 至24 小时）。随高脂肪、高热量膳食（大\n",
      "约 800-1000 卡路里；蛋白质、碳水化合物和脂肪分别提供150、250 和500-600 卡路里）一并服用本品\n",
      "对达可替尼药代动力学没有临床意义上的影响。\n",
      "分布\n",
      "达可替尼的几何平均（CV%）分布容积（Vss）为 1889 L（18%）。达可替尼与人血浆蛋白的体外结合\n",
      "率约为98%，不依赖药物浓度（250 ng/mL 至1000 ng/mL）。\n",
      "消除\n",
      "癌症患者单次口服45 mg 本品后，达可替尼的平均（CV%）血浆半衰期为70 小时（21%），达可替尼\n",
      "------------------------------\n",
      "\n",
      "[Ref2] Document: ./documents/LLD Dacomitinib - CHN - Chinese (Simplified).PDF\n",
      "Content: 尼及其活性代谢物（O-去甲基达可替尼）在血浆中的总 AUClast 增加约 6%，不认为该结果与临\n",
      "床相关。\n",
      "达可替尼对 CYP2D6 底物的影响\n",
      "与本品45 mg 单次口服给药合用，使右美沙芬（一种CYP2D6 底物）的Cmax 和AUClast 分别升高9.7 倍\n",
      "和 9.6 倍（见【药物相互作用】）。\n",
      "本品在一项I 期研究（A7471051）中评估了14 例中国健康受试者单次空腹口服本品45mg 后的药代动\n",
      "力学特征，并在一项国际多中心研究（ARCHER 1050）中评估了19 例中国患者多次口服本品45mg 后\n",
      "的药代动力学特征。上述中国人群的药代动力学结果中，未观察到与非中国人群存在明显的暴露量差\n",
      "异。\n",
      "------------------------------\n",
      "\n",
      "[Ref3] Document: ./documents/LLD Dacomitinib - CHN - Chinese (Simplified).PDF\n",
      "Content: 第1页，共12页\n",
      "Version No. : 20240229\n",
      "核准日期：2019 年05 月15 日\n",
      "修改日期：2020 年10 月16 日；2021 年 08 月09 日；2024 年 02 月29 日\n",
      "达可替尼片说明书\n",
      "                   \n",
      "  请仔细阅读说明书并在医师指导下使用\n",
      "【药品名称】\n",
      "通用名称：达可替尼片\n",
      "商品名称：多泽润®/VIZIMPRO®\n",
      "英文名称：Dacomitinib Tablets\n",
      "汉语拼音：Daketini Pian\n",
      "【成份】\n",
      "本品活性成份为达可替尼\n",
      "其化学名称为：\n",
      "(2E)-N-{4-[(3-氯-4-氟苯基)氨基]-7-甲氧基喹唑啉-6-基}-4-(哌啶-1-基)丁-2-一水烯胺\n",
      "化学结构式：\n",
      "分子式：C24H25ClFN5O2 · H2O\n",
      "分子量：487.95 \n",
      "【性状】\n",
      "本品为蓝色薄膜衣片，除去包衣后显白色或类白色。\n",
      "【适应症】\n",
      "单药用于表皮生长因子受体（EGFR）19 号外显子缺失突变或21 号外显子L858R 置换突变的局部晚期\n",
      "或转移性非小细胞肺癌（NSCLC）患者的一线治疗。\n",
      "【规格】\n",
      "(1) 15 mg; (2) 45 mg\n",
      "【用法用量】\n",
      "患者选择\n",
      "本品应在有使用经验的医疗机构中并在特定的专业技术人员指导下使用。必须基于肿瘤样本经充分验\n",
      "证的检测方法证实为EGFR 19 号外显子缺失突变或21 号外显子L858R 置换突变阳性的患者方可使用。\n",
      "推荐剂量 第2页，共12页\n",
      "Version No. : 20240229\n",
      "本品的推荐剂量为每日一次口服 45 mg，直至出现疾病进展或不可接受的毒性。本品可与食物同服，\n",
      "也可不与食物同服（见【药代动力学】）。\n",
      "每天在大致相同的时间服用本品。如果患者呕吐或漏服一剂，不应追加剂量或补充服用漏服剂量，而\n",
      "应在下一次的服药时间服用规定剂量。\n",
      "针对不良反应的剂量调整\n",
      "如果出现不良反应，按照表 1 中所述降低本品的剂量。表 2 提供了针对特定不良反应的剂量调整。\n",
      "表 1. 针对不良反应的本品推荐剂量降低方案\n",
      "剂量水平 剂量（每日一次）\n",
      "第一次剂量降低 30 mg\n",
      "第二次剂量降低 15 mg\n",
      "表 2. 针对不良反应的本品剂量调整\n",
      "不良反应 严重程度\n",
      "a\n",
      "剂量调整\n",
      "间质性肺病（ILD）（见\n",
      "【注意事项】）\n",
      "所有级别  永久停用本品。\n",
      "腹泻（见【注意事项】）2 级  暂时停用本品，直到恢复至 ≤ 1 级；然后按相同剂量水平\n",
      "继续服用本品。\n",
      " 对于再次出现的 2 级腹泻，暂时停用本品，直到恢复至 ≤ 1 \n",
      "级；降低一个剂量水平继续服用本品。\n",
      "3 或4 级  暂时停用本品，直到恢复至 ≤ 1 级；然后降低一个剂量水\n",
      "平继续服用本品。\n",
      "皮肤不良反应 （见\n",
      "【注意事项】）\n",
      "2 级  对于持续性皮肤不良反应，暂时停用本品。一旦恢复至 ≤ 1 \n",
      "级，按相同剂量水平继续服用本品。\n",
      " 对于再次出现的持续性 2 级皮肤不良反应，暂时停用本\n",
      "品，直到恢复至 ≤ 1 级，然后降低一个剂量水平继续服用\n",
      "本品。\n",
      "3 或4 级  暂时停用本品，直到恢复至 ≤ 1 级；然后降低一个剂量水\n",
      "平继续服用本品。\n",
      "其它 3 或4 级  暂时停用本品，直到恢复至 ≤ 2 级；然后降低一个剂量水\n",
      "平继续服用本品。\n",
      "a 美国国立癌症研究院通用不良事件术语标准第4.03 版\n",
      "针对使用降酸剂的剂量调整\n",
      "服用本品时，避免同时使用质子泵抑制剂（PPI）。可使用局部作用的抗酸剂或组胺 2（H2）受体拮抗\n",
      "剂代替 PPI；在服用 H2 受体拮抗剂至少 6 小时前或至少 10 小时后给予本品（见【药物相互作用】和\n",
      "【药代动力学】）。\n",
      "肾损害\n",
      "------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Query: 达可替尼片的分子式和英文名是什么\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT\n",
    "        }\n",
    "    ]\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.486361 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.832427 seconds\n"
     ]
    },
    {
     "ename": "APITimeoutError",
     "evalue": "Request timed out.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/httpx/_transports/default.py:101\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/httpcore/_sync/connection.py:78\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/httpcore/_sync/connection.py:124\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 124\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/httpcore/_backends/sync.py:215\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    214\u001b[0m         sock\u001b[38;5;241m.\u001b[39msetsockopt(\u001b[38;5;241m*\u001b[39moption)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     sock\u001b[38;5;241m.\u001b[39msetsockopt(socket\u001b[38;5;241m.\u001b[39mIPPROTO_TCP, socket\u001b[38;5;241m.\u001b[39mTCP_NODELAY, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m SyncStream(sock)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mConnectTimeout\u001b[0m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 960\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/httpx/_transports/default.py:118\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mConnectTimeout\u001b[0m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAPITimeoutError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mazure_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAZURE_OPENAI_DEPLOYMENT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m bot_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      8\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived LLM response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/openai/resources/chat/completions/completions.py:914\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    911\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    912\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    913\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/openai/_base_client.py:1247\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1235\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1242\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1243\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1244\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1245\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1246\u001b[0m     )\n\u001b[0;32m-> 1247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/openai/_base_client.py:920\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 920\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/openai/_base_client.py:969\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    966\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising timeout error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/openai/_base_client.py:1062\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/openai/_base_client.py:969\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    966\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising timeout error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/openai/_base_client.py:1062\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/epam_llm_test/lib/python3.9/site-packages/openai/_base_client.py:979\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    969\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    970\u001b[0m             input_options,\n\u001b[1;32m    971\u001b[0m             cast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    975\u001b[0m             response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    976\u001b[0m         )\n\u001b[1;32m    978\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising timeout error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 979\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    981\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAPITimeoutError\u001b[0m: Request timed out."
     ]
    }
   ],
   "source": [
    "response = azure_client.chat.completions.create(\n",
    "                \n",
    "                messages=messages,\n",
    "                max_tokens=1500,\n",
    "                temperature=0.4\n",
    "            )\n",
    "bot_message = response.choices[0].message.content.strip()\n",
    "logger.info(\"Received LLM response\")\n",
    "logger.info(f\"Response length: {len(bot_message)} characters\")\n",
    "\n",
    "formatted_message = format_response(bot_message, passage_refs, merged_chunks)\n",
    "\n",
    "        # Add bot response to conversation history\n",
    "conversation_history.append({\"role\": \"assistant\", \"content\": bot_message})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'formatted_message' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mformatted_message\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'formatted_message' is not defined"
     ]
    }
   ],
   "source": [
    "print(formatted_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epam_llm_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
